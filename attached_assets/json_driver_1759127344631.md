## JSON Ingestion

### Description

The PythonJsonDriver class is responsible for reading and writing JSON data based on a provided configuration.
It supports JSON-specific features like orient, lines, and precise_float, and allows configuration-driven ingestion and export of JSON files.
The class provides read_data and write_data methods to handle JSON files through flexible YAML-based configuration.

---

### How to Use in YAML Config

The YAML configuration defines how to read and write JSON files, supporting options like encoding, line-delimited JSON (JSONL), chunking, precision floats, and dynamic file naming using Jinja-style placeholders

### YAML Example:

```yaml
mapping:
  from:
    store:
      type: local
      input_path: <source directory path>

    data_format:
      type: json
      file_encoding: <optional encoding>  # Example: "utf-8"
      encoding_error_handling: <optional error handling>  # Example: "ignore"
      column_type: <optional data type specifications>  # Example: "id": str
      auto_convert_dates: <true/false>  # Automatically parse date columns
      use_default_date_rules: <true/false>
      timestamp_unit: <optional time unit>  # Example: "ms"
      json_structure: <optional orientation>  # Example: "records"
      column_type: <optional data type>  # Example: "frame"
      is_jsonl_format: <true/false>  # If JSON is line-delimited
      convert_labels_dtype: <true/false>  # Convert index/columns to best types
      high_precision_floats: <true/false>  # Use high precision float parsing
      read_in_chunks: <optional chunk size>  # Example: 1000
      compression_type: <optional compression>  # Example: "gzip"
      row_limit: <optional number of rows to read>  # Example: 500
      column_type: <optional engine>  # Example: "ujson"
      flat_json: <true/false>         # Optional: Whether to normalize nested JSON
      record_path: <optional path>       # Optional: Path to nested list of records
      meta: <optional meta fields>       # Optional: List of metadata fields to retain
      meta_prefix: <optional prefix>     # Optional: Prefix for meta fields
      record_prefix: <optional prefix>   # Optional: Prefix for record fields
      flatten_errors: <raise/ignore> # Optional: Error handling when meta fields missing
      flatten_sep: <optional separator> # Optional: Separator for nested keys, e.g., "."
      flatten_max_level: <optional level>        # Optional: Max depth to flatten
      force_manual_parse: <true/false> # Forces manual JSON parsing when pandas.read_json fails or its not suitable 
      selected_columns: <column mapping dictionary> # Optional: Map new column names to JSON paths for selective extraction

    entity:
      include:
        - <regex pattern to include>  # Example: "*all"
      exclude:
        - <regex pattern to exclude>  # Example: "*_test"

  to:
    store:
      type: local
      output_path: <destination directory path>

    data_format:
      type: json
      file_encoding: <optional encoding>  # Example: "utf-8"
      json_structure: <optional orientation>  # Example: "records"
      timestamp_unit: <optional unit>  # Example: "ms"
      indent: <optional value>
      file_name: "output_{${entity}$}"  # Dynamic output file naming

    

      
   
```

### Read Configuration

| Argument                     | Type     | Mandatory | Default Value | Description                                                                                           |
|------------------------------|----------|-----------|----------------|--------------------------------------------------------------------------------------------------------|
| `store.type`       | str  | Yes      | local   | Specifies the type of storage system to read files from. Options: `local` or `s3`.              |
| `store.input_path` | str  | Yes      | -       | The path to the folder or file containing the input JSON files.                                 |
| `data_format.file_encoding`       | str      | No        | `utf-8`         | Character encoding used to read the file. UTF-8 supports many characters and languages.               |
| `data_format.encoding_error_handling` | str  | No       | strict | Defines how encoding errors are handled. Options: `strict` (default), `ignore`, `replace`.     |
| `data_format.json_structure` | str | No | records | Specifies the format in which JSON data is structured. Determines how the JSON content maps to a DataFrame. Common options include:<br>• `records`: List of dictionaries, one per row<br>• `split`: Dictionary with `index`, `columns`, and `data` keys<br>• `index`: Dictionary keyed by index labels<br>• `columns`: Dictionary keyed by column labels<br>• `values`: Just the data array without labels |
| `data_format.output_type` | str | No | frame | Determines the structure of the returned data. Use `frame` to load the JSON into a Pandas DataFrame (rows and columns), which is ideal for tabular data. Use `series` when the JSON represents a single column or a one-dimensional array of values. |
| `data_format.convert_labels_dtype` | bool | No       | true     | When set to `true`, automatically converts index and column labels (e.g., string numbers or timestamps) to their most appropriate data types. Useful for improving type consistency during DataFrame operations. |
| `data_format.auto_convert_dates`      | bool  | No       | true     | If `true`, automatically detects and converts date-like string values (e.g., "2024-01-01") into Pandas `datetime64` objects. This helps in performing time-series analysis and date filtering with ease. |
| `data_format.use_default_date_rules` | bool | No | true | When set to `true`, enables Pandas' default date parsing rules during ingestion. This allows automatic recognition of common date formats (e.g., ISO 8601 strings like `"2024-01-01T00:00:00Z"`). Set to `false` if you want full control over date parsing behavior using custom logic. |
| `data_format.timestamp` | str  | No | - | Defines the column (or columns) in the JSON data to be treated as timestamps. This ensures that the specified columns are automatically converted to Pandas `datetime64` objects. It helps in handling time-based data effectively, enabling time-series analysis, sorting, and filtering operations. |
| `data_format.is_jsonal_format`              | bool  | No       | false    | If enabled, treats the input file as **JSON Lines** (each line is a separate, valid JSON object). Common in log data and streaming APIs. Set to `true` when dealing with `.jsonl` or newline-delimited JSON. |
| `data_format.high_precise_float`      | bool  | No       | false    | If set to `true`, uses high-precision parsing for floating-point numbers. Useful when working with financial data or scientific measurements where float accuracy matters. |
| `data_format.read_in_chunks`          | int   | No       | None       | Defines the number of rows to read per chunk. Useful for processing very large files in smaller, memory-efficient batches. Returns an iterator over chunks instead of a single DataFrame. |
| `data_format.compression_type`        | str   | No       | infer    | Specifies the compression algorithm used on the JSON file. Options: `gzip`, `bz2`, `zip`, `xz`, or `infer` (to guess based on file extension). |
| `data_format.json_parser_engine`             | str   | No       | ujson    | Defines the backend engine used for parsing the JSON. Options include `ujson` (UltraJSON, faster), `pyarrow` (efficient columnar format), or the default `pandas` engine. |
| `data_format.row_limit`              | int   | No       | None       | Specifies the number of rows to read from the JSON file. Useful for testing, previewing, or sampling a portion of the dataset. |
| `data_format.column_type`              | dict  | No       | None      | A dictionary defining explicit data types for specific columns. Helps override automatic type inference. Example:<br>`dtype:`<br>&nbsp;&nbsp;`id: str`<br>&nbsp;&nbsp;`created_at: datetime64[ns]` |
| `flat_json`          | `bool`                    | No        | `false`        | Enables flattening of nested JSON using `pandas.json_normalize`. If set to `true`, the JSON will be normalized into a flat table. Otherwise, it will be loaded as-is into a DataFrame. |
| `record_path`        | `str` or `list of str`    | No        | `None`         | Specifies the path to the nested array (list of records) within the JSON that should be flattened into rows. Useful when working with JSON containing embedded lists of objects. |
| `meta`               | `list of str` or `list of list` | No    | `None`         | List of fields or paths to fields to include as metadata in each record (row). These fields are repeated for each row generated from the nested structure specified in `record_path`. |
| `meta_prefix`        | `str`                     | No        | `None`         | Adds a prefix to all metadata fields (from `meta`) in the resulting DataFrame, helping to distinguish them from actual data fields. |
| `record_prefix`      | `str`                     | No        | `None`         | Adds a prefix to all flattened field names from the nested records specified by `record_path`. Helps avoid naming conflicts or improve clarity. |
| `flatten_errors`     | `'raise'` or `'ignore'`   | No        | `'raise'`      | Controls behavior if metadata fields listed in `meta` are missing from some records. `'ignore'` skips them without error; `'raise'` throws an error if any metadata field is not found. |
| `flatten_sep`        | `str`                     | No        | `"."`          | Character used to separate nested keys in the flattened column names. For example, `{"a": {"b": 1}}` becomes `a.b` if the separator is `"."`. |
| `flatten_max_level`  | `int`                     | No        | `None`         | Limits the depth of nesting to be flattened. If set to `1`, only one level of nesting will be normalized. If `None`, all levels will be flattened. |
| `force_manual_parse`  | bool  | False   | If set to `True`, the JSON file will be parsed manually using Python's built-in `json` module instead of `pandas.read_json()`. Useful for non-standard or nested JSON formats, custom error handling, or when `pandas.read_json()` fails. Especially recommended when working with JSON Lines format and needing more control over parsing behavior. |
| `selected_columns` | dict   | No        | None           | Maps new column names to JSON paths. Useful for extracting and renaming nested fields from complex JSON files. |


### Write Configuration 

| Argument          | Type   | Mandatory | Default Value | Description                                                                 |
|---------------------|--------|-----------|----------------|-----------------------------------------------------------------------------|
| `store.type`              | str    | Yes       | -              | Specifies the data format type. Should be set to `json` for JSON ingestion/export. |
| `data_format.file_encoding`     | str    | No        | `utf-8`        | Character encoding used to read/write the file. UTF-8 is a common standard supporting many characters. |
| `data_format.json_structure`    | str    | No        | `records`      | Defines the structure/format of the JSON. Options: `records`, `split`, `index`, `columns`, `values`. |
| `data_format.timestamp_unit`    | str    | No        | `s`            | Unit for parsing timestamps. Options include `s` (seconds), `ms` (milliseconds), `ns` (nanoseconds), etc. |
| `data_format.indent`            | int    | No        | `None`         | Specifies the indentation level for pretty-printing JSON output. Example: `2` will indent nested structures by 2 spaces. |
| `data_format.mode`           | str    | No        | `w`            | Specifies the I/O mode for writing data. Options: `w` (write), `a` (append). Note: `mode='a'` is supported only when `lines=True` and `orient='records'`. |
| `data_format.double_precision` | int  | No        | `10`           | Sets the number of decimal places to use when encoding floating-point numbers. Maximum allowed is 15. |


## Functionality

### Ingestion Flow

- **Read JSON**:  
  The pipeline reads JSON files from the input_path specified in the from block using the PythonJsonDriver. It supports various configurations like orient, lines, precise_float, dtype, and more to flexibly control how the JSON is parsed.

- **Process Data**:  
  After loading the data, any necessary transformations are performed according to the configuration.

- **Write CSV**:  
  The processed data is then written to the `output_path` specified in the `to` block. The filename is dynamically generated using Jinja placeholders (e.g., `{${entity}$}`).

---