# Delta Lake Ingestion Driver

## Description

The `PythonDeltaDriver` class enables robust and flexible interaction with Delta Lake tables within a configuration-driven ETL pipeline. It intelligently handles data ingestion by applying offset-based filtering based on configuration parameters and ensures consistency during writes by selecting the appropriate operation mode. While simpler operations use direct Delta Lake writes, more advanced scenarios leverage the merge API to perform transactional inserts, updates, and deletes.

In addition to its core functionality, the driver supports features like Z-order optimization for improved query performance, partitioning for efficient data layout, and configurable schema evolution strategies. It also provides support for both soft and hard delete handling, automatic vacuuming for storage cleanup, and extraction of primary key data to facilitate downstream processing. This makes the driver well-suited for production-grade pipelines requiring performance, reliability, and adaptability to evolving data structures.

---

## How to Use in YAML Config

YAML configuration for the `PythonDeltaDriver` enables flexible Delta Lake ingestion within a modular ETL pipeline.  
It supports both `full_load` (loading all data) and `incremental` (based on offset filters like `create_dttm`, `upd_dttm`) read modes to efficiently manage batch and real-time data movement.  
The driver allows various write modes including `overwrite`, `append`, `upsert`, `upsert_only`, and `append_delete`, and supports configuration of `offset_joining_condition` and primary keys for accurate merge or delete operations.

It also includes support for soft and hard deletes through the `delete_mode` option, as well as `delete_log_columns` and `update_log_columns` for tracking changes in source data.  
Additional features such as Z-order optimization, partitioning, automatic vacuuming, and key tracking help ensure optimal performance and maintainability in production-grade Delta Lake pipelines.

### YAML Example:

```yaml

              mapping_name: delta_sample          # Unique mapping identifier
              schema_evolution: True              # Enable schema evolution
              schema_evolution_type: merge        # Merge existing and new schema
              write_keys: True                    # Enable key tracking
              first_load: False
              load_type: incremental              # Perform incremental read
              write_mode: upsert                  # Use Delta merge for insert/update
              delete_type: keys                   # Use key comparison for delete detection
              delete_mode: soft_delete            # Logically delete records (hard_delete also supported)
        
              from:
                store:
                  type: local
                  path: input/                    # Path to source Delta files
                data_format:
                  type: delta
                entity:
                  include:
                    - users                       # Entities to process
                  columns_to_read:                # Read only specified columns
                    - id
                    - name
                    - age
                    - city
                  filters:                        # Filter condition before processing
                    - column: age
                      operator: ">"
                      value: 30
                  primary_keys:
                    - id                          # Required for upsert/merge
                offset:
                  joining_condition: OR           # Join offset columns with OR
                  offset_columns:
                    - create_dttm
                    - upd_dttm
        
              to:
                store:
                  type: local
                  path: output/delta_zone/{${entity}$}/data/  # Destination Delta path
                data_format:
                  type: delta
                entity:
                  name: output_{${entity}$}
                  primary_key:
                    - id
                  partition_by:
                    - city                        # Partition by column(s)
                  zorder_by:                      # Z-order optimization
                    - column: price
                      order: asc
                    - column: city
                      order: desc
                  vacuum:                         # Automatic cleanup of deleted files
                    enabled: True
                    retention_hours: 168
                  delete_log_columns:             # Mark soft deletes with a column
                    - column: DELETED
                      value: 1
                  update_log_columns:             # Mark updates with a column
                    - column: UPDATED
                      value: 1

```

### Read Configuration

| Argument                    | Type   | Mandatory         | Default Value | Description                                                                                     |
|-----------------------------|--------|-------------------|----------------|-------------------------------------------------------------------------------------------------|
| `store.type`                | str    | Yes               | local          | Storage backend to read from. Supported: `local`, `s3`, etc.                                    |
| `store.path`                | str    | Yes               | -              | Path to the Delta table location.                                                               |
| `data_format.type`          | str    | Yes               | delta          | Format of the source data. Must be `delta` for Delta Lake.                                      |
| `entity.include`            | list   | Yes               | "*all"         | List of tables/entities to include in the run. Regex patterns supported.                        |
| `entity.exclude`            | list   | No                | -              | List of tables/entities to exclude from processing.                                             |
| `entity.columns_to_read`    | list   | No                | -              | Subset of columns to read from the Delta table.                                                 |
| `entity.filters`            | list   | No                | -              | Row-level filtering before processing. Example: `age > 30`.                                     |
| `entity.primary_keys`       | list   | Yes (for merge)   | -              | Primary key columns for upsert/merge operations.                                                |
| `offset.offset_columns`     | list   | No                | -              | Columns used for incremental filtering (e.g., `create_dttm`, `upd_dttm`).                       |
| `offset.joining_condition`  | str    | No                | AND            | Whether to join offset filters using `AND` or `OR`.                                             |

---

###  Write Configuration

| Argument                         | Type   | Mandatory           | Default Value | Description                                                                                          |
|----------------------------------|--------|---------------------|----------------|------------------------------------------------------------------------------------------------------|
| `store.type`                     | str    | Yes                 | local          | Storage backend to write to. Supported: `local`, `s3`, etc.                                          |
| `store.path`                     | str    | Yes                 | -              | Output path for writing the Delta table.                                                             |
| `data_format.type`               | str    | Yes                 | delta          | Format of the output data. Must be `delta`.                                                          |
| `entity.name`                    | str    | Yes                 | -              | Target table name. Can include Jinja placeholders like `output_{${entity}$}`.                        |
| `entity.primary_key`             | list   | Yes (for upsert)    | -              | Columns used to identify rows for merge/upsert/delete operations.                                    |
| `write_mode`                     | str    | Yes                 | append         | Write strategy:<br>• `append`<br>• `overwrite`<br>• `upsert`<br>• `upsert_only`<br>• `append_delete` |
| `schema_evolution`              | bool   | No                  | False          | Enable schema evolution when writing to Delta tables.                                                |
| `schema_evolution_type`         | str    | No                  | merge            | Controls how schema differences are handled:<br>• `merge`: Adds new columns to the table<br>• `overwrite`: Replaces the entire table schema |
| `entity.partition_by`           | list   | No                  | -              | Columns used to partition the Delta table.                                                           |
| `entity.zorder_by`              | list   | No                  | -              | Columns used for Z-Ordering optimization.<br>Each entry should include `column` and `order` (asc/desc). |
| `entity.vacuum.enabled`         | bool   | No                  | False          | Enable automatic Delta vacuum to remove obsolete files.                                              |
| `entity.vacuum.retention_hours` | int    | No                  | 168            | Retention period in hours before vacuum deletes old files.                                           |
| `entity.delete_log_columns`     | list   | No                  | -              | Columns to be set for soft deletes. Example: `DELETED = 1`.                                          |
| `entity.update_log_columns`     | list   | No                  | -              | Columns to be set when rows are updated. Example: `UPDATED = 1`.                                     |

---
##  Supported Load & Write Strategies for Delta Ingestion

### Load Types

| `load_type`   | Description                                                                 |
|---------------|-----------------------------------------------------------------------------|
| `full_load`   | Loads the entire dataset every time, ignoring offsets.                      |
| `incremental` | Loads only new or changed records based on `offset_columns`.                |


### Write Modes

| `write_mode`     | Description                                                                                           |
|------------------|-------------------------------------------------------------------------------------------------------|
| `overwrite`      | Replaces the entire target table with new data.                                                       |
| `append`         | Adds new records to the target table without affecting existing data.                                 |
| `upsert`         | Inserts new records and updates existing ones using Delta Lake’s MERGE operation.                     |
| `upsert_only`    | Only updates matching records and inserts new ones; does not delete removed keys.                     |
| `append_delete`  | Appends new/changed data and deletes old records based on `primary_key` comparison.                   |


### Delete Modes (Optional)

| `delete_mode`  | Description                                                                 |
|----------------|-----------------------------------------------------------------------------|
| `soft_delete`  | Marks deleted records using a flag column (e.g., `DELETED = 1`).            |
| `hard_delete`  | Physically removes matching records from the Delta table.                  |

---
##  Functionality

### Ingestion Flow

- **Read from Delta Table**  
  The pipeline reads Delta Lake tables from the `path` specified under the `from.store` section.  
  The `PythonDeltaDriver` handles Delta-specific reading with support for:
  - Filtering (`entity.filters`)
  - Column selection (`columns_to_read`)
  - Incremental loads using `offset_columns`

- **Apply Offsets & Filters**  
  Offset logic applies dynamic filter conditions using `joining_condition` (`AND` or `OR`).  
  These filters ensure only new or changed records are loaded in `incremental` mode.

- **Process Data**  
  After loading, data can undergo transformation or validation based on mapping configuration.

- **Write to Delta Table**  
  Data is written to the path specified under `to.store`.  
  Write behavior depends on `write_mode`:
  - `overwrite`, `append`, `upsert`, `upsert_only`, or `append_delete`
  - Merge-based modes use Delta Lake’s transactional **MERGE INTO** logic.

- **Advanced Delta Features**  
  - **Z-Ordering** for optimized query performance  
  - **Partitioning** for scalable writes  
  - **Vacuum** to clean up old versions  
  - **Schema Evolution** to handle column changes automatically
---
## Delta Lake Schema Evolution

Schema evolution in Delta Lake allows seamless updates to a table’s schema to accommodate changing incoming data. It enables pipelines to adapt automatically during append or overwrite operations, adding new columns without manual intervention. Schema evolution is a feature that allows users to easily change a table's current schema to accommodate data that is changing over time. Most commonly, it is used when performing an append or overwrite operation to automatically adapt the schema to include one or more new columns. Two modes are supported: **merge**, which adds new columns while preserving the existing schema, and **overwrite**, which replaces the entire schema.

---

## YAML Configuration Parameters

```yaml
schema_evolution: true
schema_mode: merge         # Options: merge, overwrite, None

```

## Schema Evolution Configuration Options

| Parameter           | Type    | Required | Default | Description                                                                 |
|---------------------|---------|----------|---------|-----------------------------------------------------------------------------|
| `schema_evolution`  | Boolean | Yes      | false   | Enables schema evolution logic during Delta write operations.              |
| `schema_mode`       | String  | No       | null    | Controls how schema changes are applied:<br>• `merge`<br>• `overwrite`<br>• `None` (disabled). |

---

## Supported `schema_mode` Values

### `merge`

- **Safely adds** new columns from the incoming data into the existing Delta table schema.
- Does **not drop or alter** existing columns or data types.
- Ideal for use cases where the schema evolves over time and backward compatibility must be maintained.
- Prevents data loss by extending, not replacing, the schema.

**Example Behavior:**
```text
Incoming data has a new column `discount`.
→ The `discount` column is added to the Delta table.
```
### `overwrite`

- **Replaces the entire table schema** with the schema from the incoming data.
- Can **drop existing columns** and **alter data types** to match the incoming data.
- Use this mode with **extreme caution** in production environments, as it may cause **loss of data or metadata**.
- Most useful for scenarios where the table schema must be reset or fully redefined (e.g., during reprocessing or structural refactor).

**Example Behavior:**
```text
Existing schema: id, name, price, discount  
Incoming schema: id, name, price  
→ Table schema becomes: id, name, price  
(discount column is dropped)
```
### `None` (or omit the parameter)

- **Disables all schema evolution**.
- Requires that the incoming schema **exactly matches** the existing Delta table schema.
- Will raise **errors** if new columns are added or if existing columns have changed data types.
- Best suited for **stable production pipelines** where strict schema enforcement is expected and schema drift is not allowed.

---

## Best Practices

- Use `schema_mode: merge` for pipelines where **new columns may be introduced over time**, and **backward compatibility** is needed.
- Use `schema_mode: overwrite` only when you **intentionally want to reset or replace** the schema—ideally in **development or reprocessing workflows**.
- Avoid setting `schema_evolution: true` without a valid `schema_mode`; leaving it `None` **disables schema handling**.
- Always **validate the incoming schema** before applying `overwrite` to avoid **unintentional data loss**.

---

## Example Configuration

```yaml
schema_evolution: true
schema_evolution_type: merge
load_type: full_load
write_mode: append
```
