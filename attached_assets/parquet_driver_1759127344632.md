## Parquet Ingestion

### Description

The `PythonParquetDriver` class is responsible for reading and writing Parquet files using configurable options.  
It leverages the `read_data` method to load Parquet files into a DataFrame, and the `write_data` method to save DataFrames to Parquet format.

This class supports common configurations such as compression, indexing, filtering, column selection, partitioning, and engine choices.


---

### How to Use in YAML Config

The YAML configuration allows users to specify how Parquet files should be read and written â€” including paths, selected columns, partitions, engine type, and more.


### YAML Example:

```yaml
mapping:
  from:
    store:
      type: local
      path: <source directory path>

    data_format:
      type: parquet
      engine: <optional engine>  # Example: "pyarrow", "fastparquet"
      columns:
        - <optional column name>  # Example: "Restaurant"
        - <optional column name>  # Example: "City"
      storage_options: <optional storage settings>  # Example: {"anon": true}
      use_nullable_dtypes: <true/false>  # Example: true
      filters:
        - - <column>  # Example: "City"
          - <operator>  # Example: "=="
          - <value>  # Example: "Kochi"
      dtype_backend: <optional backend>  # Example: "pyarrow"

    entity:
      include:
        - <regex pattern to include>  # Example: "*"

  to:
    store:
      type: local
      path: <destination directory path>

    data_format:
      type: parquet
      engine: <optional engine>  # Example: "pyarrow"
      compression: <optional compression>  # Example: "snappy", "gzip"
      index: <true/false>  # Whether to write index or not
      partition_cols:
        - <optional column to partition by>  # Example: "City"
        - <optional column to partition by>  # Example: "Restaurant"
      storage_options: <optional storage settings>  # Example: {"anon": true}

    entity:
      name: output_{${entity}$}
   
```

### Read Configuration (Parquet)

| Argument                           | Type     | Mandatory | Default Value            | Description                                                                                         |
|------------------------------------|----------|-----------|--------------------------|-----------------------------------------------------------------------------------------------------|
| `store.type`                       | str      | Yes       | local                    | Type of storage location to read input files from. Supported values: `local`, `s3`.                |
| `store.input_path`                 | str      | Yes       | -                        | Path to the source directory or file. Example: `/data/input/`                                      |
| `data_format.type`                 | str      | Yes       | parquet                  | Format of the input data. For this config, must be set to `parquet`.                               |
| `data_format.engine`              | str      | No        | auto                     | Parquet engine to use. Supported: `"pyarrow"`, `"fastparquet"`.                                    |
| `data_format.columns`             | list     | No        | -                        | List of columns to read from the file. If not specified, all columns are read.                     |
| `data_format.filters`             | list     | No        | -                        | Apply row-level filters. Example: `[[ "City", "==", "Kochi" ]]`                                    |
| `data_format.storage_options`     | dict     | No        | -                        | Storage options for remote sources (like S3). Example: `{"anon": true}`                            |
| `data_format.use_nullable_dtypes` | bool     | No        | False                    | If `True`, pandas will use nullable dtypes where appropriate.                                      |
| `data_format.dtype_backend`       | str      | No        | -                        | Backend for data types. Example: `"pyarrow"`                                                        |
| `entity.include`                  | list     | Yes       | "*all"                   | List of files or regex patterns to include. Use `"*"` to include all files.                        |

### Write Configuration (Parquet)

| Argument                         | Type   | Mandatory | Default Value        | Description                                                                                       |
|----------------------------------|--------|------------|-----------------------|---------------------------------------------------------------------------------------------------|
| `store.type`                    | str    | Yes        | local                | Type of storage to write output files. Example: `local` or `s3`.                                 |
| `store.output_path`             | str    | Yes        | -                    | Path where Parquet output files will be saved. Supports dynamic paths like `output/{${entity}$}`.|
| `data_format.type`              | str    | Yes        | parquet              | Output file format. Must be set to `parquet`.                                                     |
| `data_format.engine`            | str    | No         | auto                 | Engine used to write Parquet files. Common options: `pyarrow`, `fastparquet`.                    |
| `data_format.compression`       | str    | No         | snappy               | Compression method. Options include `snappy`, `gzip`, `brotli`, `none`.                          |
| `data_format.columns`           | list   | No         | -                    | List of columns to include in the output. Leave blank to write all columns.                      |
| `data_format.index`             | bool   | No         | False                | Whether to include the DataFrame index in the output file.                                       |
| `data_format.partition_cols`    | list   | No         | -                    | Columns used to partition the output Parquet data into subfolders.                               |
| `data_format.storage_options`   | dict   | No         | `{}`                 | Additional storage options for cloud stores like S3 (e.g., credentials, configs).                |
| `data_format.use_nullable_dtypes`| bool  | No         | True                 | Use nullable dtypes when writing (especially for pandas >= 1.0).                                 |
| `data_format.dtype_backend`     | str    | No         | pyarrow              | Backend used for data type conversion, mainly `pyarrow`.                                         |
| `data_format.file_name`         | str    | Yes        | output_{${entity}$}  | Output file name pattern. `${entity}` will be replaced dynamically with the entity name.         |




## Functionality

### Ingestion Flow

- **Read Parquet**:  
  The pipeline reads the Parquet files from the `input_path` defined in the `from` block using the appropriate Parquet engine (e.g., `pyarrow` or `fastparquet`). It applies any specified options such as selected `columns`, filters, and data types during ingestion.

- **Process Data**:  
  Once the data is loaded, transformations or filtering operations are applied as configured. This step prepares the data for output.

- **Write Parquet**:  
  The transformed data is written to the `output_path` defined in the `to` block. Output options such as compression (`snappy`, `gzip`, etc.), partitioning columns, index inclusion, and file naming (`output_{${entity}$}`) are respected. The engine and backend for writing are chosen based on the configuration.


---
